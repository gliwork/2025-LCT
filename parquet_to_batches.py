"""
parquet_to_batches.py
Модуль для подготовки батчей в формате Pytorch из имеющихся файлов данных по ИТП в Parquet (для снижения объема хранения)
Utilities to create PyTorch-ready batches from Parquet files generated earlier.

Main classes/functions:
- build_itp_mapping(metadata_json_path) -> {itp_uuid: int}
- build_sample_index(parquet_paths, itp_filter=None, m=14, n=3, stride_hours=1, start_ts=None, end_ts=None)
    -> returns list of sample descriptors (itp_uuid, t0_timestamp)
- ParquetITPDataset(sample_index, parquet_paths, itp_map, m, n, holiday_func, temp_column='temperature', ...)
    -> PyTorch Dataset yielding dicts compatible with collate_fn from model script.
- example usage at bottom.

Requirements:
  pip install pandas pyarrow python-dateutil torch lru-dict python-holidays (optional)
"""

import os
import json
from typing import List, Dict, Tuple, Callable, Optional, Any
from functools import lru_cache
from datetime import datetime, timedelta, date
import numpy as np
import pandas as pd
import pyarrow.dataset as ds
import pyarrow as pa
from dateutil import parser as dateparse
import math

# optional: python-holidays package for holiday calendar
try:
    import holidays
    HOLIDAYS_AVAILABLE = True
except Exception:
    HOLIDAYS_AVAILABLE = False

# lightweight LRU cache for loaded ITP time series (per itp_uuid)
from collections import OrderedDict


class SimpleLRUCache:
    def __init__(self, max_items: int = 128):
        self.max_items = max_items
        self._od = OrderedDict()

    def get(self, key):
        if key in self._od:
            self._od.move_to_end(key)
            return self._od[key]
        return None

    def set(self, key, value):
        self._od[key] = value
        self._od.move_to_end(key)
        if len(self._od) > self.max_items:
            self._od.popitem(last=False)


# ---------------------------
# Helpers: holiday id mapping
# ---------------------------
def default_holiday_func_factory(country: str = "US", prov: Optional[str] = None):
    """
    Returns a function date -> holiday_id (int).
    Uses python-holidays if available. If not available, returns 0 for non-holiday, 1 for weekend.
    You can also pass your own holiday_func(date)->int.
    """
    if HOLIDAYS_AVAILABLE:
        cal = holidays.CountryHoliday(country, prov=prov) if prov else holidays.CountryHoliday(country)
        def holiday_func(d: date) -> int:
            # 0 = normal day, 1 = public holiday, 2 = weekend (if not holiday)
            if d in cal:
                return 1
            if d.weekday() >= 5:
                return 2
            return 0
        return holiday_func
    else:
        def fallback(d: date) -> int:
            return 2 if d.weekday() >= 5 else 0
        return fallback


# ---------------------------
# Build ITP map
# ---------------------------
def build_itp_map_from_metadata(metadata_json_path: str) -> Dict[str, int]:
    """
    metadata_json_path: file path to itp_metadata.json generated by the generator script
    Returns dict mapping itp_uuid (string) -> integer index (0..N-1)
    """
    with open(metadata_json_path, "r", encoding="utf-8") as f:
        metadata = json.load(f)
    itp_map = {}
    for i, rec in enumerate(metadata):
        itp_map[rec["itp_uuid"]] = i
    return itp_map


# ---------------------------
# Build sample index (descriptors)
# ---------------------------
def build_sample_index(parquet_paths: List[str],
                       m: int,
                       n: int,
                       stride_hours: int = 24,
                       start_ts: Optional[str] = None,
                       end_ts: Optional[str] = None,
                       itp_filter: Optional[List[str]] = None,
                       timestamp_col: str = "ts") -> List[Dict[str, Any]]:
    """
    Scan Parquet files (fast via pyarrow.dataset) and collect valid sample start times t0 per ITP.
    For each ITP, we require availability of data:
      - history: last m days (i.e., m*24 hourly rows) strictly before t0 (from t0 - m*24 to t0-1 hour)
      - future: next n days for target (t0 .. t0 + n*24 -1)
    stride_hours: spacing between candidate t0 (e.g., 24 -> daily windows). Use 1 for hourly windows.
    Returns list of dict: {"itp_uuid": str, "t0": pd.Timestamp}
    NOTE: This is a scanning operation; for many files it may take a while but uses dataset filters.
    """
    ds_iter = ds.dataset(parquet_paths, format="parquet", partitioning="hive")
    # build predicate to scan only itp_uuid in itp_filter
    if start_ts is None:
        start_ts = "2024-01-01T00:00:00"
    if end_ts is None:
        end_ts = "2025-12-31T23:00:00"

    # We'll read only ts and itp_uuid columns initially to build ranges per ITP
    table = ds_iter.to_table(columns=["itp_uuid", timestamp_col])
    df = table.to_pandas()
    # convert ts to pandas datetime (if not already)
    df[timestamp_col] = pd.to_datetime(df[timestamp_col])

    if itp_filter is not None:
        df = df[df["itp_uuid"].isin(itp_filter)]

    # group by itp_uuid and compute available ts range
    sample_index = []
    min_t = pd.to_datetime(start_ts)
    max_t = pd.to_datetime(end_ts)
    grouped = df.groupby("itp_uuid")
    required_history_hours = m * 24
    required_future_hours = n * 24

    for itp, g in grouped:
        ts_sorted = g[timestamp_col].sort_values()
        if ts_sorted.empty:
            continue
        # create set for faster membership tests (could be heavy), but we only need continuous ranges => use min/max
        first_ts = ts_sorted.min()
        last_ts = ts_sorted.max()
        # earliest valid t0 is first_ts + required_history_hours
        earliest_t0 = first_ts + pd.Timedelta(hours=required_history_hours)
        latest_t0 = last_ts - pd.Timedelta(hours=required_future_hours)
        if latest_t0 < earliest_t0:
            continue
        # sample t0s with stride
        t0 = earliest_t0.ceil(freq=f"{stride_hours}H")
        while t0 <= latest_t0 and t0 <= max_t:
            if t0 >= min_t:
                sample_index.append({"itp_uuid": itp, "t0": t0})
            t0 = t0 + pd.Timedelta(hours=stride_hours)
    return sample_index


# ---------------------------
# Dataset
# ---------------------------
class ParquetITPDataset:
    """
    PyTorch-friendly dataset that yields samples built from Parquet hourly rows.

    Constructor args:
      - sample_index: list of dicts {"itp_uuid": str, "t0": pd.Timestamp}
      - parquet_paths: list of parquet files or folder (pyarrow.dataset will handle)
      - itp_map: dict itp_uuid -> int
      - m, n: history days and forecast days
      - cache_size: number of ITPs to keep in in-memory cache (each as pandas DataFrame)
      - columns names: specify names used in Parquet (flow_col, temp_col, consumer_type_col, network_segment_col)
      - holiday_func: function date->int holiday id
      - fill_method: 'interpolate' or 'ffill' for missing hourly values inside windows
    """

    def __init__(self,
                 sample_index: List[Dict[str, Any]],
                 parquet_paths: List[str],
                 itp_map: Dict[str, int],
                 m: int,
                 n: int,
                 flow_col: str = "cold_flow_m3_per_hour",
                 temp_col: Optional[str] = None,
                 consumer_col: Optional[str] = "consumer_type",
                 consumer_id_col: Optional[str] = "consumer_type_id",
                 segment_col: Optional[str] = None,
                 timestamp_col: str = "ts",
                 cache_size: int = 256,
                 holiday_func: Optional[Callable[[date], int]] = None,
                 fill_method: str = "interpolate"):
        self.samples = sample_index
        self.parquet_paths = parquet_paths
        self.itp_map = itp_map
        self.m = m
        self.n = n
        self.flow_col = flow_col
        self.temp_col = temp_col
        self.consumer_col = consumer_col
        self.consumer_id_col = consumer_id_col
        self.segment_col = segment_col
        self.timestamp_col = timestamp_col
        self.cache = SimpleLRUCache(max_items=cache_size)
        self.dataset = ds.dataset(parquet_paths, format="parquet")
        self.holiday_func = holiday_func or default_holiday_func_factory()
        self.fill_method = fill_method

    def __len__(self):
        return len(self.samples)

    # internal: load full history for an ITP into pandas DataFrame (hourly)
    def _load_itp_df(self, itp_uuid: str) -> pd.DataFrame:
        cached = self.cache.get(itp_uuid)
        if cached is not None:
            return cached
        # Use dataset scan with filter for this itp_uuid
        expr = (ds.field("itp_uuid") == itp_uuid)
        table = self.dataset.to_table(filter=expr)
        df = table.to_pandas()
        # ensure timestamp col
        df[self.timestamp_col] = pd.to_datetime(df[self.timestamp_col])
        # set index
        df = df.set_index(self.timestamp_col).sort_index()
        # keep only columns we need
        keep_cols = [self.flow_col]
        if self.temp_col and self.temp_col in df.columns:
            keep_cols.append(self.temp_col)
        if self.consumer_col and self.consumer_col in df.columns:
            keep_cols.append(self.consumer_col)
        if self.consumer_id_col and self.consumer_id_col in df.columns:
            keep_cols.append(self.consumer_id_col)
        if self.segment_col and self.segment_col in df.columns:
            keep_cols.append(self.segment_col)
        df = df[keep_cols].copy()
        # cache
        self.cache.set(itp_uuid, df)
        return df

    def _get_window_arrays(self, itp_df: pd.DataFrame, t0: pd.Timestamp) -> Tuple[np.ndarray, Optional[np.ndarray], Dict[str, Any]]:
        """
        Build hist_matrix (1,24,m), avg_daily_temps_m, and meta info.
        Returns (hist_matrix, avg_daily_temps_m_or_None, meta_dict)
        meta_dict contains consumer_type_id, consumer_type, network_segment (if exist)
        """
        # history start/end
        hist_end = t0 - pd.Timedelta(hours=1)
        hist_start = t0 - pd.Timedelta(hours=self.m * 24)
        # target end
        target_start = t0
        target_end = t0 + pd.Timedelta(hours=self.n * 24 - 1)

        # slice history and future rows
        # Reindex to continuous hourly index to detect missing hours
        idx_hist = pd.date_range(start=hist_start, end=hist_end, freq="H")
        hist_slice = itp_df.reindex(idx_hist)

        # fill missing using interpolation or ffill/backfill
        if self.fill_method == "interpolate":
            if self.flow_col in hist_slice.columns:
                hist_slice[self.flow_col] = hist_slice[self.flow_col].interpolate(limit=24, limit_direction="both")
            # temps if present
            if self.temp_col and self.temp_col in hist_slice.columns:
                hist_slice[self.temp_col] = hist_slice[self.temp_col].interpolate(limit=24, limit_direction="both")
        else:
            if self.flow_col in hist_slice.columns:
                hist_slice[self.flow_col] = hist_slice[self.flow_col].ffill().bfill()
            if self.temp_col and self.temp_col in hist_slice.columns:
                hist_slice[self.temp_col] = hist_slice[self.temp_col].ffill().bfill()

        # if still NaNs (too many missing) -> fill zeros (or choose other policy)
        hist_slice[self.flow_col] = hist_slice[self.flow_col].fillna(0.0)
        if self.temp_col and self.temp_col in hist_slice.columns:
            hist_slice[self.temp_col] = hist_slice[self.temp_col].fillna(method="ffill").fillna(0.0)

        # Build hist_matrix 24 x m
        # We want columns as days in chronological order (oldest -> newest)
        arr = hist_slice[self.flow_col].values.reshape(self.m, 24)  # shape (m,24) if hourly contiguous
        # But reshape above gives rows = days; we need (24, m) with hours in rows
        mat_24_m = arr.T.copy()  # (24, m)
        hist_matrix = mat_24_m[np.newaxis, :, :]  # (1,24,m)

        # avg_daily_temps_m: compute means per day if temp available
        avg_daily_temps = None
        if self.temp_col and self.temp_col in hist_slice.columns:
            temps_arr = hist_slice[self.temp_col].values.reshape(self.m, 24)  # (m,24)
            avg_daily_temps = temps_arr.mean(axis=1)  # length m
        # meta: consumer_type_id, consumer_type, network_segment from last available row
        meta = {"consumer_type": None, "consumer_type_id": None, "network_segment": None}
        # get last non-null row from itp_df up to hist_end
        last_row = itp_df.loc[:hist_end].dropna(how="all")
        if not last_row.empty:
            last = last_row.iloc[-1]
            if self.consumer_col and self.consumer_col in last_row.columns:
                meta["consumer_type"] = last.get(self.consumer_col, None)
            if self.consumer_id_col and self.consumer_id_col in last_row.columns:
                meta["consumer_type_id"] = int(last.get(self.consumer_id_col, None)) if pd.notnull(last.get(self.consumer_id_col, None)) else None
            if self.segment_col and self.segment_col in last_row.columns:
                meta["network_segment"] = last.get(self.segment_col, None)
        return hist_matrix.astype(np.float32), (avg_daily_temps.astype(np.float32) if avg_daily_temps is not None else None), meta

    def _get_target_vector(self, itp_df: pd.DataFrame, t0: pd.Timestamp) -> np.ndarray:
        """
        Return vector length n*24 with hourly flows from t0 to t0 + n*24 -1.
        If some hours missing -> fill with interpolation/bfill/ffill as in history.
        """
        tgt_idx = pd.date_range(start=t0, end=t0 + pd.Timedelta(hours=self.n*24 - 1), freq="H")
        tgt_slice = itp_df.reindex(tgt_idx)
        if self.fill_method == "interpolate":
            tgt_slice[self.flow_col] = tgt_slice[self.flow_col].interpolate(limit=24, limit_direction="both")
        else:
            tgt_slice[self.flow_col] = tgt_slice[self.flow_col].ffill().bfill()
        tgt_slice[self.flow_col] = tgt_slice[self.flow_col].fillna(0.0)
        return tgt_slice[self.flow_col].values.astype(np.float32)

    def _build_holiday_seq(self, t0: pd.Timestamp) -> np.ndarray:
        """
        Build holiday id sequence of length m + n:
         - days: most recent m days prior to t0 (hist days) in chronological order (oldest->newest)
         - followed by n future days starting at t0 (day of t0 included)
        returns np.array (m+n,) dtype=int
        """
        # hist days start date:
        hist_start_date = (t0 - pd.Timedelta(hours=self.m * 24)).date()
        seq = []
        for d in range(self.m):
            dt = hist_start_date + timedelta(days=d)
            seq.append(int(self.holiday_func(dt)))
        # future days
        future_start_date = t0.date()
        for d in range(self.n):
            dt = future_start_date + timedelta(days=d)
            seq.append(int(self.holiday_func(dt)))
        return np.array(seq, dtype=np.int64)

    def __getitem__(self, idx):
        rec = self.samples[idx]
        itp_uuid = rec["itp_uuid"]
        t0 = rec["t0"]
        itp_df = self._load_itp_df(itp_uuid)
        hist_matrix, avg_daily_temps, meta = self._get_window_arrays(itp_df, t0)
        target = self._get_target_vector(itp_df, t0)
        # features scalars
        hour_start = np.array([[t0.hour / 23.0]], dtype=np.float32)  # normalize by 23
        dow = np.array([[t0.weekday() / 6.0]], dtype=np.float32)
        week_of_year = np.array([[t0.isocalendar()[1] / 52.0]], dtype=np.float32)
        # lunar day (approximate using simple lunar cycle)
        # approximate lunar day from known new moon reference — but here we approximate by 29.53-day cycle from 2000-01-06 new moon
        def approximate_lunar_day(dt: date):
            ref = date(2000, 1, 6)
            days = (dt - ref).days
            lunar = int((days % 29.53) + 1)
            return lunar
        lunar_day_val = approximate_lunar_day(t0.date())
        lunar_day = np.array([[lunar_day_val / 30.0]], dtype=np.float32)

        # avg_daily_temps_m as (m,) numeric; if None => zeros
        if avg_daily_temps is None:
            avg_daily_temps = np.zeros(self.m, dtype=np.float32)
        # holiday seq
        holiday_seq = self._build_holiday_seq(t0)  # (m+n,)

        # map itp_uuid to integer id
        itp_id = np.int64(self.itp_map.get(itp_uuid, -1))
        consumer_type_id = np.int64(meta.get("consumer_type_id", -1) if meta.get("consumer_type_id", None) is not None else -1)
        # network_segment_id left as -1 if unknown
        network_segment_id = np.int64(-1)
        # if network segment exists as a column and value maps, you can produce integer mapping externally

        sample = {
            "hist_matrix": hist_matrix,                    # (1,24,m)
            "hour_start": hour_start,                      # (1,1)
            "dow": dow,                                    # (1,1)
            "week_of_year": week_of_year,                  # (1,1)
            "lunar_day": lunar_day,                        # (1,1)
            "avg_daily_temps_m": avg_daily_temps,          # (m,)
            "holiday_seq_mn": holiday_seq,                 # (m+n,)
            "itp_id": itp_id,                              #
            "consumer_type_id": consumer_type_id,
            "network_segment_id": network_segment_id,
            "target": target                               # (n*24,)
        }
        return sample


# ---------------------------
# Utilities: collate_fn for PyTorch DataLoader
# ---------------------------
import torch

def dataset_collate_fn(batch_samples: List[Dict[str, Any]]):
    """
    Batch the dicts produced by ParquetITPDataset into tensors compatible with model.
    """
    B = len(batch_samples)
    # hist_matrix: (B,1,24,m)
    hist_list = [torch.tensor(s["hist_matrix"], dtype=torch.float32) for s in batch_samples]
    hist = torch.stack(hist_list, dim=0)
    hour = torch.tensor(np.vstack([s["hour_start"] for s in batch_samples]), dtype=torch.float32)
    dow = torch.tensor(np.vstack([s["dow"] for s in batch_samples]), dtype=torch.float32)
    week = torch.tensor(np.vstack([s["week_of_year"] for s in batch_samples]), dtype=torch.float32)
    lunar = torch.tensor(np.vstack([s["lunar_day"] for s in batch_samples]), dtype=torch.float32)
    temps = torch.tensor(np.vstack([s["avg_daily_temps_m"] for s in batch_samples]), dtype=torch.float32)
    holidays = torch.tensor(np.vstack([s["holiday_seq_mn"] for s in batch_samples]), dtype=torch.long)
    itp_ids = torch.tensor([s["itp_id"] for s in batch_samples], dtype=torch.long)
    consumer_ids = torch.tensor([s["consumer_type_id"] for s in batch_samples], dtype=torch.long)
    seg_ids = torch.tensor([s["network_segment_id"] for s in batch_samples], dtype=torch.long)
    targets = torch.tensor(np.vstack([s["target"] for s in batch_samples]), dtype=torch.float32)
    return {
        "hist_matrix": hist,
        "hour_start": hour,
        "dow": dow,
        "week_of_year": week,
        "lunar_day": lunar,
        "avg_daily_temps_m": temps,
        "holiday_seq_mn": holidays,
        "itp_id": itp_ids,
        "consumer_type_id": consumer_ids,
        "network_segment_id": seg_ids,
        "target": targets
    }


# ---------------------------
# Example usage
# ---------------------------
if __name__ == "__main__":
    # minimal example: point this to your output dir from generator
    parquet_dir = "./synthetic_itps_out"
    metadata_json = os.path.join(parquet_dir, "itp_metadata.json")
    # gather parquet files (you may have many; pass list)
    parquet_files = [os.path.join(parquet_dir, f) for f in os.listdir(parquet_dir) if f.endswith(".parquet")]

    # build itp map
    itp_map = build_itp_map_from_metadata(metadata_json)

    # build sample index: use m=14 (past 14 days) and n=3 (forecast 3 days), stride 24 -> daily windows
    m = 14
    n = 3
    sample_index = build_sample_index(parquet_files, m=m, n=n, stride_hours=24)

    print(f"Found {len(sample_index)} samples")

    # construct dataset
    holiday_func = default_holiday_func_factory(country="US")  # or your country
    ds_parquet = ParquetITPDataset(sample_index, parquet_files, itp_map, m=m, n=n,
                                   temp_col=None,  # if you have temperature column use its name
                                   cache_size=256,
                                   holiday_func=holiday_func,
                                   fill_method="interpolate")

    # test fetch first sample
    s0 = ds_parquet[0]
    print("hist_matrix shape:", s0["hist_matrix"].shape)
    print("holiday_seq length:", len(s0["holiday_seq_mn"]))
    print("avg_daily_temps_m shape:", s0["avg_daily_temps_m"].shape)
    print("target shape:", s0["target"].shape)

    # create PyTorch DataLoader
    from torch.utils.data import DataLoader
    loader = DataLoader(ds_parquet, batch_size=16, shuffle=True, collate_fn=dataset_collate_fn, num_workers=0)
    batch = next(iter(loader))
    print("Batch hist shape:", batch["hist_matrix"].shape)  # (B,1,24,m)
    print("Batch target shape:", batch["target"].shape)     # (B, n*24)
